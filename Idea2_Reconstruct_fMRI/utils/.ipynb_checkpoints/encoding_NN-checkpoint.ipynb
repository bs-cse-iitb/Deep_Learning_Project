{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc961a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import numpy as np\n",
    "from himalaya.backend import set_backend\n",
    "from himalaya.ridge import RidgeCV\n",
    "from himalaya.scoring import correlation_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbeff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python ridge.py --target blip --roi early ventral midventral midlateral lateral parietal  --subject subj01\n",
    "#python ridge.py --target init_latent --roi early --subject subj01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05dee7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'z'\n",
    "#roi = ['early', 'ventral', 'midventral', 'midlateral','lateral','parietal']\n",
    "roi = ['early']\n",
    "subject = 'subj01'\n",
    "backend = set_backend(\"numpy\", on_error=\"warn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd38441",
   "metadata": {},
   "outputs": [],
   "source": [
    "mridir = f'../../mrifeat/{subject}/'\n",
    "featdir = '../../nsdfeat/subjfeat/'\n",
    "savedir = f'../..//fmri/{subject}/'\n",
    "os.makedirs(savedir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "499fe859",
   "metadata": {},
   "outputs": [],
   "source": [
    "if target == 'c' or target == 'z': # CVPR\n",
    "    alpha = [0.000001,0.00001,0.0001,0.001,0.01, 0.1, 1]\n",
    "else: # text / GAN / depth decoding (with much larger number of voxels)\n",
    "    alpha = [10000, 20000, 40000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ff41243",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas=alpha)\n",
    "\n",
    "preprocess_pipeline = make_pipeline(\n",
    "    StandardScaler(with_mean=True, with_std=True),\n",
    ")\n",
    "pipeline = make_pipeline(\n",
    "    preprocess_pipeline,\n",
    "    ridge,\n",
    ")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00ee62a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = []\n",
    "Y_te = []\n",
    "for croi in roi:\n",
    "    if 'conv' in target: # We use averaged features for GAN due to large number of dimension of features\n",
    "        cX = np.load(f'{mridir}/{subject}_{croi}_betas_ave_tr.npy').astype(\"float32\")\n",
    "    else:\n",
    "        cX = np.load(f'{mridir}/{subject}_{croi}_betas_tr.npy').astype(\"float32\")\n",
    "    cX_te = np.load(f'{mridir}/{subject}_{croi}_betas_ave_te.npy').astype(\"float32\")\n",
    "    Y.append(cX)\n",
    "    Y_te.append(cX_te)\n",
    "Y = np.hstack(Y)\n",
    "Y_te = np.hstack(Y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d356142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load(f'{featdir}/{subject}_each_{target}_tr.npy').astype(\"float32\").reshape([Y.shape[0],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88c6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = np.load(f'{featdir}/{subject}_ave_{target}_te.npy').astype(\"float32\").reshape([Y_te.shape[0],-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c831d566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24980, 5917)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "794da66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 5917)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e776a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "083dddd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now making decoding model for... subj01:  ['early'], z\n",
      "X (24980, 4096), Y (24980, 5917), X_te (982, 4096), Y_te (982, 5917)\n"
     ]
    }
   ],
   "source": [
    "print(f'Now making decoding model for... {subject}:  {roi}, {target}')\n",
    "print(f'X {X.shape}, Y {Y.shape}, X_te {X_te.shape}, Y_te {Y_te.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ef3f374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mfit(X, Y)\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/sklearn/pipeline.py:427\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    426\u001b[0m         fit_params_last_step \u001b[38;5;241m=\u001b[39m fit_params_steps[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 427\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator\u001b[38;5;241m.\u001b[39mfit(Xt, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_last_step)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/backend/_utils.py:97\u001b[0m, in \u001b[0;36mforce_cpu_backend.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;66;03m# skip if the object does not force cpu use\u001b[39;00m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforce_cpu\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mforce_cpu:\n\u001b[0;32m---> 97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# set corresponding cpu backend\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     original_backend \u001b[38;5;241m=\u001b[39m get_backend()\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/ridge/_sklearn_api.py:321\u001b[0m, in \u001b[0;36mRidgeCV.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    318\u001b[0m cv \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# ------------------ call the solver\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m tmp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_solver(X\u001b[38;5;241m=\u001b[39mX, Y\u001b[38;5;241m=\u001b[39my, cv\u001b[38;5;241m=\u001b[39mcv, alphas\u001b[38;5;241m=\u001b[39malphas,\n\u001b[1;32m    322\u001b[0m                         fit_intercept\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept,\n\u001b[1;32m    323\u001b[0m                         Y_in_cpu\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mY_in_cpu)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_intercept:\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_alphas_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv_scores_ \u001b[38;5;241m=\u001b[39m tmp[:\u001b[38;5;241m3\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/ridge/_sklearn_api.py:44\u001b[0m, in \u001b[0;36m_BaseRidge._call_solver\u001b[0;34m(self, **direct_params)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m intersection:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParameters \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m should not be given in solver_params, since \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthey are either fixed or have a direct parameter in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m     42\u001b[0m         (intersection, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdirect_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msolver_params)\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/ridge/_random_search.py:510\u001b[0m, in \u001b[0;36msolve_ridge_cv_svd\u001b[0;34m(X, Y, alphas, fit_intercept, score_func, cv, local_alpha, n_targets_batch, n_targets_batch_refit, n_alphas_batch, conservative, Y_in_cpu, warn)\u001b[0m\n\u001b[1;32m    499\u001b[0m fixed_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(return_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    500\u001b[0m                     concentration\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, jitter_alphas\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    501\u001b[0m                     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, n_iter\u001b[38;5;241m=\u001b[39mn_iter, warn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    503\u001b[0m copied_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(alphas\u001b[38;5;241m=\u001b[39malphas, score_func\u001b[38;5;241m=\u001b[39mscore_func, cv\u001b[38;5;241m=\u001b[39mcv,\n\u001b[1;32m    504\u001b[0m                      local_alpha\u001b[38;5;241m=\u001b[39mlocal_alpha, fit_intercept\u001b[38;5;241m=\u001b[39mfit_intercept,\n\u001b[1;32m    505\u001b[0m                      n_targets_batch\u001b[38;5;241m=\u001b[39mn_targets_batch,\n\u001b[1;32m    506\u001b[0m                      n_targets_batch_refit\u001b[38;5;241m=\u001b[39mn_targets_batch_refit,\n\u001b[1;32m    507\u001b[0m                      n_alphas_batch\u001b[38;5;241m=\u001b[39mn_alphas_batch,\n\u001b[1;32m    508\u001b[0m                      conservative\u001b[38;5;241m=\u001b[39mconservative, Y_in_cpu\u001b[38;5;241m=\u001b[39mY_in_cpu)\n\u001b[0;32m--> 510\u001b[0m tmp \u001b[38;5;241m=\u001b[39m solve_group_ridge_random_search([X], Y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_params,\n\u001b[1;32m    511\u001b[0m                                       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfixed_params)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fit_intercept:\n\u001b[1;32m    514\u001b[0m     deltas, coefs, cv_scores, intercept \u001b[38;5;241m=\u001b[39m tmp\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/ridge/_random_search.py:216\u001b[0m, in \u001b[0;36msolve_group_ridge_random_search\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    213\u001b[0m     Xtrain \u001b[38;5;241m=\u001b[39m X_[train] \u001b[38;5;241m-\u001b[39m Xtrain_mean\n\u001b[1;32m    214\u001b[0m     Xtest \u001b[38;5;241m=\u001b[39m X_[test] \u001b[38;5;241m-\u001b[39m Xtrain_mean\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m matrix, alpha_batch \u001b[38;5;129;01min\u001b[39;00m _decompose_ridge(\n\u001b[1;32m    217\u001b[0m         Xtrain\u001b[38;5;241m=\u001b[39mXtrain, alphas\u001b[38;5;241m=\u001b[39malphas, negative_eigenvalues\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    218\u001b[0m         n_alphas_batch\u001b[38;5;241m=\u001b[39mn_alphas_batch, method\u001b[38;5;241m=\u001b[39mdiagonalize_method):\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# n_alphas_batch, n_features, n_samples_train = \\\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# matrix.shape\u001b[39;00m\n\u001b[1;32m    221\u001b[0m     matrix \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mmatmul(Xtest, matrix)\n\u001b[1;32m    222\u001b[0m     \u001b[38;5;66;03m# n_alphas_batch, n_samples_test, n_samples_train = \\\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;66;03m# matrix.shape\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/ridge/_random_search.py:388\u001b[0m, in \u001b[0;36m_decompose_ridge\u001b[0;34m(Xtrain, alphas, n_alphas_batch, method, negative_eigenvalues)\u001b[0m\n\u001b[1;32m    384\u001b[0m     n_alphas_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(alphas)\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;66;03m# SVD: X = U @ np.diag(eigenvalues) @ Vt\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m     U, eigenvalues, Vt \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39msvd(Xtrain, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown method=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (method, ))\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/himalaya/backend/numpy.py:207\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(X, full_matrices)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msvd\u001b[39m(X, full_matrices\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_scipy:\n\u001b[0;32m--> 207\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m linalg\u001b[38;5;241m.\u001b[39msvd(X, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    210\u001b[0m         UsV_list \u001b[38;5;241m=\u001b[39m [linalg\u001b[38;5;241m.\u001b[39msvd(Xi, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices) \u001b[38;5;28;01mfor\u001b[39;00m Xi \u001b[38;5;129;01min\u001b[39;00m X]\n",
      "File \u001b[0;32m~/anaconda3/envs/DeepLearning/lib/python3.11/site-packages/scipy/linalg/_decomp_svd.py:127\u001b[0m, in \u001b[0;36msvd\u001b[0;34m(a, full_matrices, compute_uv, overwrite_a, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m    123\u001b[0m lwork \u001b[38;5;241m=\u001b[39m _compute_lwork(gesXd_lwork, a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], a1\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    124\u001b[0m                        compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# perform decomposition\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m u, s, v, info \u001b[38;5;241m=\u001b[39m gesXd(a1, compute_uv\u001b[38;5;241m=\u001b[39mcompute_uv, lwork\u001b[38;5;241m=\u001b[39mlwork,\n\u001b[1;32m    128\u001b[0m                       full_matrices\u001b[38;5;241m=\u001b[39mfull_matrices, overwrite_a\u001b[38;5;241m=\u001b[39moverwrite_a)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSVD did not converge\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pipeline.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5d6c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pipeline.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cef4a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-871.7748 ,  381.46893,  709.52094, ...,   66.04494,  -68.46825,\n",
       "        106.3498 ], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a10e61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2989.,   -13., -1054., ...,  -194.,  -395.,  -321.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_te[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8080d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction accuracy is: 0.0193\n"
     ]
    }
   ],
   "source": [
    "rs = correlation_score(Y_te.T,scores.T)\n",
    "print(f'Prediction accuracy is: {np.mean(rs):3.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc7e06cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(f'{savedir}/{subject}_{\"_\".join(roi)}_scores_{target}.npy',scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d18ea498",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{savedir}reconstruct_fmri_model.pkl', 'wb') as f:\n",
    "    pickle.dump(pipeline, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ce7019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8384ec66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/781 [00:00<?, ?it/s]100%|██████████| 781/781 [00:05<00:00, 133.05it/s]\n",
      "100%|██████████| 781/781 [00:05<00:00, 131.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F  # Parameterless functions, like (some) activation functions\n",
    "import torchvision.datasets as datasets  # Standard datasets\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset for augmentation\n",
    "from torch import optim  # For optimizers like SGD, Adam, etc.\n",
    "from torch import nn  # All neural network modules\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.utils.data import (\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment by creating mini batches etc.\n",
    "from tqdm import tqdm  # For nice progress bar!\n",
    "\n",
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "\n",
    "\n",
    "        super(NN, self).__init__()\n",
    "        # Our first linear layer take input_size, in this case 784 nodes to 50\n",
    "        # and our second linear layer takes 50 to the num_classes we have, in\n",
    "        # this case 10.\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc4 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Set device cuda for GPU if it's available otherwise run on the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 4096\n",
    "num_classes = 5917\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create dataset from several tensors with matching first dimension\n",
    "# Samples will be drawn from the first dimension (rows)\n",
    "\n",
    "# Load Data\n",
    "train_dataset = TensorDataset( torch.Tensor(X), torch.Tensor(Y) )\n",
    "test_dataset = TensorDataset( torch.Tensor(X_te), torch.Tensor(Y_te) )\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize network\n",
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train Network\n",
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(tqdm(train_loader)):\n",
    "        # Get data to cuda if possible\n",
    "        data = data.to(device=device)\n",
    "        targets = targets.to(device=device)\n",
    "\n",
    "        #data = data.reshape(data.shape[0], -1)\n",
    "\n",
    "        # Forward\n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "def check_accuracy(loader, model):\n",
    "    num_correct = 0\n",
    "    num_samples = 0\n",
    "    model.eval()\n",
    "\n",
    "    # We don't need to keep track of gradients here so we wrap it in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        # Loop through the data\n",
    "        for x, y in loader:\n",
    "\n",
    "            # Move data to device\n",
    "            x = x.to(device=device)\n",
    "            y = y.to(device=device)\n",
    "\n",
    "            # Get to correct shape\n",
    "            x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "            # Forward pass\n",
    "            scores = model(x)\n",
    "            _, predictions = scores.max(1)\n",
    "\n",
    "            # Check how many we got correct\n",
    "            num_correct += (predictions == y).sum()\n",
    "\n",
    "            # Keep track of number of samples\n",
    "            num_samples += predictions.size(0)\n",
    "\n",
    "    model.train()\n",
    "    return num_correct / num_samples\n",
    "\n",
    "\n",
    "# Check accuracy on training & test to see how good our model\n",
    "# print(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n",
    "# print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a0f42b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a29a1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915559c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa51ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bc8034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb74e15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca1f5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5e3ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833d3c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6a838e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5db000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125f85a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b5706fde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{50: 20.662,\n",
       " 100: 14.156,\n",
       " 200: 19.762,\n",
       " 500: 16.52,\n",
       " 1000: 36.862,\n",
       " 1200: 45.06,\n",
       " 2000: 53.264}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_to_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24806a8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475381c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f34462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3132db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a339da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d7868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd869785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fc1f15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
